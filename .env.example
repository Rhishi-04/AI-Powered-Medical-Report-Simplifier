# Ollama Configuration (Local LLM - No API key needed!)
OLLAMA_URL=http://localhost:11434

# Model Configuration
# Available models you have: llama3.2:latest, deepseek-r1:latest, mistral:latest, qwen2.5-coder:1.5b
# Recommended: llama3.2:latest (best balance of speed and quality)
LLM_MODEL_NAME=llama3.2:latest
LLM_MAX_TOKENS=2048
LLM_TEMPERATURE=0.1

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=True

# OCR Configuration
OCR_CONFIDENCE_THRESHOLD=0.6

# Validation Configuration
VALIDATION_CONFIDENCE_THRESHOLD=0.7

# Logging
LOG_LEVEL=INFO
